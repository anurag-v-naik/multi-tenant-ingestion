# ğŸ—ï¸ Multi-Tenant Data Ingestion Framework

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![Terraform](https://img.shields.io/badge/terraform-1.0+-purple.svg)](https://www.terraform.io/)
[![Databricks](https://img.shields.io/badge/databricks-enterprise-orange.svg)](https://databricks.com/)
[![AWS](https://img.shields.io/badge/AWS-cloud-orange.svg)](https://aws.amazon.com/)

A production-ready, enterprise-grade multi-tenant data ingestion framework built with Databricks, Unity Catalog, Apache Iceberg, and AWS. Designed for organizations that need secure, scalable, and cost-effective data processing with complete tenant isolation.

## ğŸ¯ Key Features

- **ğŸ¢ Multi-Tenant Architecture**: Complete isolation between organizations with dedicated resources
- **ğŸ”¥ Databricks Integration**: PySpark execution with organization-specific workspaces
- **ğŸ“Š Unity Catalog**: Centralized metadata management with tenant governance
- **ğŸ§Š Iceberg Tables**: Cross-platform interoperability (Snowflake, Redshift, BigQuery)
- **ğŸ”’ Enterprise Security**: End-to-end encryption, RBAC, and compliance automation
- **ğŸ’° Cost Management**: Detailed chargeback, resource quotas, and optimization
- **ğŸš€ Auto-Scaling**: Dynamic resource allocation based on workload patterns
- **ğŸ“ˆ Monitoring**: Comprehensive observability and alerting

## ğŸ—ï¸ Architecture Overview

```mermaid
graph TB
    subgraph "Multi-Tenant UI Layer"
        UI[React Multi-Tenant Dashboard]
        API[API Gateway with Tenant Routing]
    end
    
    subgraph "Microservices Layer"
        PS[Pipeline Service]
        CS[Connector Service]
        DQS[Data Quality Service]
        CAS[Catalog Service]
        NS[Notification Service]
    end
    
    subgraph "Processing Layer - Databricks"
        DB1[Finance Workspace]
        DB2[Retail Workspace]
        DB3[Healthcare Workspace]
    end
    
    subgraph "Data Layer"
        UC[Unity Catalog]
        ICE[Iceberg Tables]
        S3A[Finance S3 Bucket]
        S3B[Retail S3 Bucket]
        S3C[Healthcare S3 Bucket]
    end
    
    subgraph "Cross-Platform Integration"
        SF[Snowflake]
        RS[Redshift]
        BQ[BigQuery]
    end
    
    UI --> API
    API --> PS
    API --> CS
    API --> DQS
    API --> CAS
    
    PS --> DB1
    PS --> DB2
    PS --> DB3
    
    DB1 --> UC
    DB2 --> UC
    DB3 --> UC
    
    UC --> ICE
    ICE --> S3A
    ICE --> S3B
    ICE --> S3C
    
    ICE --> SF
    ICE --> RS
    ICE --> BQ
```

## ğŸš€ Quick Start

### Prerequisites

- **AWS Account** with administrative access
- **Databricks Account** (Premium or Enterprise tier)
- **Terraform** >= 1.0
- **Docker** >= 20.0
- **Python** >= 3.9
- **Node.js** >= 16.0

### 1. Clone Repository

```bash
git clone https://github.com/anurag-v-naik/multi-tenant-ingestion.git
cd multi-tenant-ingestion-framework
```

### 2. Infrastructure Setup

```bash
# Configure AWS credentials
aws configure

# Copy and edit Terraform variables
cp infrastructure/terraform/terraform.tfvars.example infrastructure/terraform/terraform.tfvars
nano infrastructure/terraform/terraform.tfvars

# Deploy infrastructure
cd infrastructure/terraform
terraform init
terraform plan
terraform apply -auto-approve
```

### 3. Application Deployment

```bash
# Return to root directory
cd ../../

# Build and deploy services
./deployment/scripts/deploy.sh --build-images --deploy-services

# Setup organizations
./deployment/scripts/setup-organization.sh finance
./deployment/scripts/setup-organization.sh retail
./deployment/scripts/setup-organization.sh healthcare
```

### 4. Verify Deployment

```bash
# Check service health
curl https://$(terraform output -raw load_balancer_dns)/health

# Access the UI
open https://$(terraform output -raw load_balancer_dns)
```

## ğŸ“ Repository Structure

```
multi-tenant-ingestion-framework/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â”œâ”€â”€ ci.yml                     # Continuous Integration
â”‚       â”œâ”€â”€ cd.yml                     # Continuous Deployment
â”‚       â””â”€â”€ security-scan.yml          # Security scanning
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ ARCHITECTURE.md                # Detailed architecture guide
â”‚   â”œâ”€â”€ DEPLOYMENT.md                  # Deployment guide
â”‚   â”œâ”€â”€ API_REFERENCE.md               # API documentation
â”‚   â”œâ”€â”€ USER_GUIDE.md                  # End-user guide
â”‚   â””â”€â”€ CONTRIBUTING.md                # Contribution guidelines
â”œâ”€â”€ infrastructure/
â”‚   â”œâ”€â”€ terraform/
â”‚   â”‚   â”œâ”€â”€ main.tf                    # Complete AWS infrastructure
â”‚   â”‚   â”œâ”€â”€ variables.tf               # Configuration variables
â”‚   â”‚   â”œâ”€â”€ outputs.tf                 # Infrastructure outputs
â”‚   â”‚   â””â”€â”€ terraform.tfvars.example   # Example configuration
â”‚   â”œâ”€â”€ kubernetes/
â”‚   â”‚   â”œâ”€â”€ namespaces/                # Organization namespaces
â”‚   â”‚   â”œâ”€â”€ ingress/                   # Multi-tenant ingress
â”‚   â”‚   â””â”€â”€ monitoring/                # Observability stack
â”‚   â””â”€â”€ databricks/
â”‚       â”œâ”€â”€ workspace-config.py        # Workspace automation
â”‚       â””â”€â”€ unity-catalog-setup.sql    # Catalog initialization
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ pipeline-service/
â”‚   â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”‚   â”œâ”€â”€ main.py                # FastAPI application
â”‚   â”‚   â”‚   â”œâ”€â”€ models/                # Database models
â”‚   â”‚   â”‚   â”œâ”€â”€ api/                   # API endpoints
â”‚   â”‚   â”‚   â””â”€â”€ core/                  # Business logic
â”‚   â”‚   â””â”€â”€ notebooks/
â”‚   â”‚       â””â”€â”€ pyspark_template.py    # Databricks notebook template
â”‚   â”œâ”€â”€ catalog-service/
â”‚   â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”‚   â””â”€â”€ app/
â”‚   â”‚       â”œâ”€â”€ main.py                # Unity Catalog & Iceberg service
â”‚   â”‚       â”œâ”€â”€ models/
â”‚   â”‚       â”œâ”€â”€ api/
â”‚   â”‚       â””â”€â”€ core/
â”‚   â”œâ”€â”€ connector-service/
â”‚   â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”‚   â””â”€â”€ app/
â”‚   â”‚       â”œâ”€â”€ main.py                # Connector registry
â”‚   â”‚       â”œâ”€â”€ connectors/            # Built-in connectors
â”‚   â”‚       â””â”€â”€ templates/             # Connector templates
â”‚   â”œâ”€â”€ data-quality-service/
â”‚   â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”‚   â””â”€â”€ app/
â”‚   â”‚       â”œâ”€â”€ main.py                # DQ validation service
â”‚   â”‚       â”œâ”€â”€ rules/                 # Quality rules engine
â”‚   â”‚       â””â”€â”€ reports/               # DQ reporting
â”‚   â””â”€â”€ ui/
â”‚       â”œâ”€â”€ Dockerfile
â”‚       â”œâ”€â”€ package.json
â”‚       â”œâ”€â”€ src/
â”‚       â”‚   â”œâ”€â”€ components/            # React components
â”‚       â”‚   â”œâ”€â”€ pages/                 # Application pages
â”‚       â”‚   â”œâ”€â”€ hooks/                 # Custom hooks
â”‚       â”‚   â””â”€â”€ utils/                 # Utility functions
â”‚       â””â”€â”€ public/
â”œâ”€â”€ deployment/
â”‚   â”œâ”€â”€ docker-compose.yml             # Local development
â”‚   â”œâ”€â”€ helm-charts/                   # Kubernetes deployment
â”‚   â”‚   â”œâ”€â”€ multi-tenant-ingestion/
â”‚   â”‚   â””â”€â”€ monitoring/
â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â”œâ”€â”€ deploy.sh                  # Main deployment script
â”‚   â”‚   â”œâ”€â”€ setup-organization.sh      # Organization setup
â”‚   â”‚   â”œâ”€â”€ backup.sh                  # Backup automation
â”‚   â”‚   â””â”€â”€ rollback.sh                # Rollback procedures
â”‚   â””â”€â”€ configs/
â”‚       â”œâ”€â”€ production.env             # Production configuration
â”‚       â”œâ”€â”€ staging.env                # Staging configuration
â”‚       â””â”€â”€ development.env            # Development configuration
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ pipeline-configs/              # Sample pipeline configurations
â”‚   â”œâ”€â”€ connectors/                    # Custom connector examples
â”‚   â”œâ”€â”€ notebooks/                     # Sample Databricks notebooks
â”‚   â””â”€â”€ data-quality-rules/            # DQ rule examples
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/                          # Unit tests
â”‚   â”œâ”€â”€ integration/                   # Integration tests
â”‚   â”œâ”€â”€ load/                          # Performance tests
â”‚   â””â”€â”€ fixtures/                      # Test data
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ setup-dev-environment.sh       # Development setup
â”‚   â”œâ”€â”€ generate-test-data.py          # Test data generation
â”‚   â””â”€â”€ migrate-organization.py        # Organization migration
â”œâ”€â”€ .env.example                       # Environment variables template
â”œâ”€â”€ .gitignore                         # Git ignore rules
â”œâ”€â”€ .pre-commit-config.yaml           # Pre-commit hooks
â”œâ”€â”€ docker-compose.yml                # Local development stack
â”œâ”€â”€ LICENSE                           # MIT License
â”œâ”€â”€ Makefile                          # Development commands
â”œâ”€â”€ pyproject.toml                    # Python project configuration
â”œâ”€â”€ requirements.txt                  # Python dependencies
â””â”€â”€ README.md                         # This file
```

## ğŸ”§ Configuration

### Environment Variables

Create a `.env` file based on `.env.example`:

```bash
# AWS Configuration
AWS_REGION=us-east-1
AWS_ACCOUNT_ID=123456789012

# Databricks Configuration
DATABRICKS_ACCOUNT_ID=your-databricks-account-id
DATABRICKS_HOST=https://your-workspace.cloud.databricks.com

# Database Configuration
DATABASE_URL=postgresql://user:pass@localhost:5432/multi_tenant_ingestion
REDIS_URL=redis://localhost:6379/0

# Security
JWT_SECRET_KEY=your-jwt-secret-key
ENCRYPTION_KEY=your-encryption-key

# Feature Flags
UNITY_CATALOG_ENABLED=true
ICEBERG_ENABLED=true
MULTI_TENANT_MODE=true
```

### Organization Configuration

```yaml
# terraform.tfvars
organizations = {
  "finance" = {
    name             = "finance"
    display_name     = "Finance Department"
    cost_center      = "FIN-001"
    compliance_level = "high"
    resource_quotas = {
      max_dbu_per_hour         = 100
      max_storage_gb           = 10000
      max_api_calls_per_minute = 1000
    }
  }
  "retail" = {
    name             = "retail"
    display_name     = "Retail Division"
    cost_center      = "RET-001"
    compliance_level = "medium"
    resource_quotas = {
      max_dbu_per_hour         = 200
      max_storage_gb           = 50000
      max_api_calls_per_minute = 2000
    }
  }
}
```

## ğŸ³ Local Development

### Using Docker Compose

```bash
# Start all services locally
docker-compose up -d

# View logs
docker-compose logs -f

# Stop services
docker-compose down
```

### Using Make Commands

```bash
# Setup development environment
make setup-dev

# Run tests
make test

# Format code
make format

# Build images
make build

# Deploy locally
make deploy-local
```

## ğŸ§ª Testing

### Run All Tests

```bash
# Unit tests
pytest tests/unit/ -v

# Integration tests
pytest tests/integration/ -v

# Load tests
pytest tests/load/ -v

# Coverage report
pytest --cov=services/ --cov-report=html
```

### Specific Test Categories

```bash
# Test multi-tenant isolation
pytest tests/integration/test_tenant_isolation.py -v

# Test Databricks integration
pytest tests/integration/test_databricks.py -v

# Test Unity Catalog functionality
pytest tests/integration/test_unity_catalog.py -v

# Test Iceberg interoperability
pytest tests/integration/test_iceberg.py -v
```

## ğŸ“Š Monitoring & Observability

### Health Checks

```bash
# Service health
curl http://localhost:8000/health

# Database connectivity
curl http://localhost:8000/health/database

# Databricks connectivity
curl http://localhost:8000/health/databricks

# Unity Catalog connectivity
curl http://localhost:8000/health/unity-catalog
```

### Metrics & Dashboards

- **Grafana Dashboard**: `http://localhost:3000`
- **Prometheus Metrics**: `http://localhost:9090`
- **CloudWatch Dashboard**: Available in AWS Console
- **Application Logs**: Available in CloudWatch Logs

## ğŸ” Security

### Authentication & Authorization

```bash
# Generate JWT token for testing
curl -X POST http://localhost:8000/auth/login \
  -H "Content-Type: application/json" \
  -d '{"username": "admin", "password": "admin", "organization_id": "finance"}'

# Use token in API calls
curl -H "Authorization: Bearer $JWT_TOKEN" \
     -H "X-Organization-ID: finance" \
     http://localhost:8000/api/v1/pipelines
```

### Secrets Management

```bash
# Create organization secrets
aws secretsmanager create-secret \
  --name "finance/databricks-token" \
  --secret-string '{"token": "dapi-xxx", "workspace_url": "https://finance.databricks.com"}'

# Rotate secrets
./scripts/rotate-secrets.sh finance
```

## ğŸ’° Cost Management

### Resource Monitoring

```bash
# Check organization costs
./scripts/cost-report.sh finance

# Set resource quotas
./scripts/set-quota.sh finance --max-dbu=100 --max-storage=10000

# Generate cost allocation report
./scripts/chargeback-report.sh --month=2024-01
```

## ğŸš€ Deployment

### Production Deployment

```bash
# Deploy to production
./deployment/scripts/deploy.sh \
  --environment=production \
  --build-images \
  --deploy-services \
  --run-migrations

# Verify deployment
./deployment/scripts/verify-deployment.sh production
```

### Staging Deployment

```bash
# Deploy to staging
./deployment/scripts/deploy.sh \
  --environment=staging \
  --skip-migrations

# Run integration tests
./deployment/scripts/run-integration-tests.sh staging
```

## ğŸ“š Documentation

- [Architecture Guide](docs/ARCHITECTURE.md) - Detailed system architecture
- [Deployment Guide](docs/DEPLOYMENT.md) - Step-by-step deployment instructions
- [API Reference](docs/API_REFERENCE.md) - Complete API documentation
- [User Guide](docs/USER_GUIDE.md) - End-user documentation
- [Contributing Guide](docs/CONTRIBUTING.md) - How to contribute

## ğŸ¤ Contributing

We welcome contributions! Please see our [Contributing Guide](docs/CONTRIBUTING.md) for details.

### Development Workflow

1. Fork the repository
2. Create a feature branch: `git checkout -b feature/amazing-feature`
3. Make your changes and add tests
4. Run the test suite: `make test`
5. Commit your changes: `git commit -m 'Add amazing feature'`
6. Push to the branch: `git push origin feature/amazing-feature`
7. Submit a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ†˜ Support

- **Issues**: [GitHub Issues](https://github.com/your-org/multi-tenant-ingestion-framework/issues)
- **Discussions**: [GitHub Discussions](https://github.com/your-org/multi-tenant-ingestion-framework/discussions)
- **Documentation**: [docs/](docs/)
- **Email**: data-engineering@yourcompany.com

## ğŸ·ï¸ Tags

`data-engineering` `multi-tenant` `databricks` `unity-catalog` `iceberg` `aws` `terraform` `pyspark` `data-pipeline` `enterprise` `microservices` `cloud-native`

---

â­ **Star this repository** if you find it helpful!

**Built with â¤ï¸ by the Data Engineering Team**
