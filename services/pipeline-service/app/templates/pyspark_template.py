"""PySpark notebook template for data pipelines."""

PYSPARK_TEMPLATE = """
# Databricks notebook source
# MAGIC %md
# MAGIC # Data Pipeline: {pipeline_name}
# MAGIC 
# MAGIC **Organization:** {organization_id}
# MAGIC **Pipeline ID:** {pipeline_id}
# MAGIC **Generated:** Auto-generated by Multi-Tenant Framework

# COMMAND ----------

# MAGIC %md
# MAGIC ## Setup and Configuration

# COMMAND ----------

import json
import uuid
from datetime import datetime, timezone
from pyspark.sql import SparkSession, DataFrame
from pyspark.sql.functions import *
from pyspark.sql.types import *
from delta.tables import DeltaTable

# Initialize Spark session with optimizations
spark = SparkSession.builder \\
    .appName("MultiTenant-{pipeline_name}") \\
    .config("spark.sql.adaptive.enabled", "true") \\
    .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \\
    .config("spark.sql.adaptive.skewJoin.enabled", "true") \\
    .config("spark.databricks.delta.optimizeWrite.enabled", "true") \\
    .config("spark.databricks.delta.autoCompact.enabled", "true") \\
    .getOrCreate()

# Set log level
spark.sparkContext.setLogLevel("WARN")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Pipeline Configuration

# COMMAND ----------

# Pipeline metadata
ORGANIZATION_ID = "{organization_id}"
PIPELINE_ID = "{pipeline_id}"
PIPELINE_NAME = "{pipeline_name}"
RUN_ID = dbutils.widgets.get("run_id") if dbutils.widgets.get("run_id") else str(uuid.uuid4())

# Source configuration
SOURCE_CONFIG = {source_config}

# Target configuration  
TARGET_CONFIG = {target_config}

# Transformation configuration
TRANSFORMATION_CONFIG = {transformation_config}

print(f"Starting pipeline run: {{RUN_ID}}")
print(f"Organization: {{ORGANIZATION_ID}}")
print(f"Pipeline: {{PIPELINE_NAME}}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Data Quality Functions

# COMMAND ----------

def validate_data_quality(df: DataFrame, rules: dict) -> dict:
    \"\"\"Validate data quality based on rules.\"\"\"
    results = {{
        "total_records": df.count(),
        "quality_checks": []
    }}
    
    for rule in rules.get("rules", []):
        rule_type = rule.get("type")
        
        if rule_type == "completeness":
            for column in rule.get("columns", []):
                if column in df.columns:
                    null_count = df.filter(col(column).isNull()).count()
                    completeness = 1 - (null_count / results["total_records"])
                    threshold = rule.get("threshold", 0.95)
                    
                    results["quality_checks"].append({{
                        "rule": f"completeness_{{column}}",
                        "value": completeness,
                        "threshold": threshold,
                        "passed": completeness >= threshold
                    }})
        
        elif rule_type == "uniqueness":
            for column in rule.get("columns", []):
                if column in df.columns:
                    distinct_count = df.select(column).distinct().count()
                    uniqueness = distinct_count / results["total_records"]
                    threshold = rule.get("threshold", 1.0)
                    
                    results["quality_checks"].append({{
                        "rule": f"uniqueness_{{column}}",
                        "value": uniqueness,
                        "threshold": threshold,
                        "passed": uniqueness >= threshold
                    }})
    
    return results

def log_metrics(metrics: dict):
    \"\"\"Log pipeline metrics.\"\"\"
    # Log to Databricks metrics
    for key, value in metrics.items():
        if isinstance(value, (int, float)):
            spark.sparkContext.statusTracker().addExecutorMetrics(key, value)

# COMMAND ----------

# MAGIC %md
# MAGIC ## Source Data Extraction

# COMMAND ----------

def extract_source_data() -> DataFrame:
    \"\"\"Extract data from source based on configuration.\"\"\"
    source_type = SOURCE_CONFIG.get("type")
    
    if source_type == "delta":
        # Delta Lake source
        table_path = SOURCE_CONFIG["parameters"]["table_path"]
        df = spark.read.format("delta").load(table_path)
        
    elif source_type == "mysql":
        # MySQL source
        connection_props = {{
            "user": SOURCE_CONFIG["parameters"]["username"],
            "password": SOURCE_CONFIG["parameters"]["password"],
            "driver": "com.mysql.cj.jdbc.Driver"
        }}
        
        jdbc_url = f"jdbc:mysql://{{SOURCE_CONFIG['parameters']['host']}}:{{SOURCE_CONFIG['parameters']['port']}}/
        jdbc_url = f"jdbc:mysql://{SOURCE_CONFIG['parameters']['host']}:{SOURCE_CONFIG['parameters']['port']}/{SOURCE_CONFIG['parameters']['database']}"
        table_name = SOURCE_CONFIG["parameters"]["table"]
     
    df = spark.read.jdbc(jdbc_url, table_name, properties=connection_props)
    
    # Handle incremental loading
    if "incremental_column" in SOURCE_CONFIG["parameters"]:
        incremental_column = SOURCE_CONFIG["parameters"]["incremental_column"]
        last_value = get_last_processed_value(incremental_column)
        if last_value:
            df = df.filter(col(incremental_column) > last_value)

elif source_type == "s3":
    # S3 source
    file_path = SOURCE_CONFIG["parameters"]["file_path"]
    file_format = SOURCE_CONFIG["parameters"].get("format", "parquet")
    
    if file_format == "parquet":
        df = spark.read.parquet(file_path)
    elif file_format == "csv":
        df = spark.read.option("header", "true").csv(file_path)
    elif file_format == "json":
        df = spark.read.json(file_path)
    else:
        raise ValueError(f"Unsupported file format: {file_format}")

elif source_type == "kafka":
    # Kafka streaming source
    kafka_config = SOURCE_CONFIG["parameters"]
    df = spark.readStream \\
        .format("kafka") \\
        .option("kafka.bootstrap.servers", kafka_config["bootstrap_servers"]) \\
        .option("subscribe", kafka_config["topic"]) \\
        .option("startingOffsets", kafka_config.get("starting_offsets", "latest")) \\
        .load()
    
    # Parse Kafka value as JSON
    df = df.select(
        col("key").cast("string"),
        from_json(col("value").cast("string"), kafka_config["schema"]).alias("data"),
        col("timestamp")
    ).select("key", "data.*", "timestamp")

else:
    raise ValueError(f"Unsupported source type: {source_type}")

print(f"Extracted {df.count() if not df.isStreaming else 'streaming'} records from {source_type} source")
return df
